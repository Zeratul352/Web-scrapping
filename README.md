# Web scrapping practice: Data Extraction, Processing, and Reporting

This repository contains the solutions for the web scrapping lab, which focused on various aspects of data extraction, processing, analysis, and reporting using Python. The lab covered techniques for web scraping, interacting with APIs, parsing different data formats (JSON, XML, plain text), performing data analysis, and exporting results to structured files like JSON and CSV.

#### Task 1: Extract specific information from a Wikipedia page and save it to a JSON file.
#### Task 2: Fetch maximum temperature data for Tokyo from the Open-Meteo API and save it to a JSON file.
#### Task 3: Load and analyze complex weather data from a JSON file, generating detailed daily and summary reports.
#### Task 4: Load, summarize, and export complex weather data from a JSON file to a CSV file.
#### Task 5: Parse weather data from an XML file and store key metrics in a CSV file.
#### Task 6: Extract weather data from a plain text file using regular expressions and store it in a CSV file.
#### Task 7: Use unittest to ensure workflow of the code.
### Skills Obtained
Through the completion, gained practical experience and reinforced understanding in the following key areas:
- Web Scraping Fundamentals: Proficiently fetching HTML content and extracting specific elements using requests and BeautifulSoup.
- API Interaction: Making HTTP requests to external APIs (requests) and handling JSON responses.
- JSON: Loading, parsing, extracting, and serializing data to JSON files.
- XML: Parsing XML documents (xml.etree.ElementTree) and extracting structured information.
- Regular Expressions: Utilizing re module for pattern-based data extraction from unstructured text.
- Data Cleaning & Preprocessing: Handling header rows, splitting strings, and basic data type conversions.
- Unit Testing: Writing and executing unit tests using pytest to ensure function correctness and reliability.
### Result
This lab provided a comprehensive hands-on experience in the data engineering pipeline, from raw data acquisition to structured reporting, utilizing a variety of Python libraries and techniques.
